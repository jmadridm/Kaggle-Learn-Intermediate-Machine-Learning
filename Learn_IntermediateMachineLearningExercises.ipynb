{"cells":[{"metadata":{},"cell_type":"markdown","source":"# [Introduction](https://www.kaggle.com/kernels/fork/3370272)\n\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n---\n\nAs a warm-up, you'll review some machine learning fundamentals and submit your initial results to a Kaggle competition.\n\n# Setup\n\nThe questions below will give you feedback on your work. Run the following cell to set up the feedback system."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\"\"\"\n# Set up code checking\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.ml_intermediate.ex1 import *\nprint(\"Setup Complete\")\n\"\"\"","execution_count":1,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"'\\n# Set up code checking\\nfrom learntools.core import binder\\nbinder.bind(globals())\\nfrom learntools.ml_intermediate.ex1 import *\\nprint(\"Setup Complete\")\\n'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"You will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) to predict home prices in Iowa using 79 explanatory variables describing (almost) every aspect of the homes.  \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation features in `X_train` and `X_valid`, along with the prediction targets in `y_train` and `y_valid`.  The test features are loaded in `X_test`.  (_If you need to review **features** and **prediction targets**, please check out [this short tutorial](https://www.kaggle.com/dansbecker/your-first-machine-learning-model).  To read about model **validation**, look [here](https://www.kaggle.com/dansbecker/model-validation).  Alternatively, if you'd prefer to look through a full course to review all of these topics, start [here](https://www.kaggle.com/learn/machine-learning).)_"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')\n\n# Obtain target and predictors\ny = X_full.SalePrice\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = X_full[features].copy()\nX_test = X_test_full[features].copy()\n\n# Break off validation set from trainig data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the next cell to print the first several rows of the data. It's a nice way to get an overview of the data you will use in your price prediction model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"     LotArea  YearBuilt  1stFlrSF  2ndFlrSF  FullBath  BedroomAbvGr  \\\nId                                                                    \n619    11694       2007      1828         0         2             3   \n871     6600       1962       894         0         1             2   \n93     13360       1921       964         0         1             2   \n818    13265       2002      1689         0         2             3   \n303    13704       2001      1541         0         2             3   \n\n     TotRmsAbvGrd  \nId                 \n619             9  \n871             5  \n93              5  \n818             7  \n303             6  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LotArea</th>\n      <th>YearBuilt</th>\n      <th>1stFlrSF</th>\n      <th>2ndFlrSF</th>\n      <th>FullBath</th>\n      <th>BedroomAbvGr</th>\n      <th>TotRmsAbvGrd</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>619</th>\n      <td>11694</td>\n      <td>2007</td>\n      <td>1828</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>6600</td>\n      <td>1962</td>\n      <td>894</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>13360</td>\n      <td>1921</td>\n      <td>964</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>818</th>\n      <td>13265</td>\n      <td>2002</td>\n      <td>1689</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>303</th>\n      <td>13704</td>\n      <td>2001</td>\n      <td>1541</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Evaluate several models\n\nThe next code cell defines five different random forest models.  Run this code cell without changes.  (_To review **random forests**, look [here](https://www.kaggle.com/dansbecker/random-forests)._)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Define the models\nmodel_1 = RandomForestRegressor(n_estimators=50,random_state=0)\nmodel_2 = RandomForestRegressor(n_estimators=100,random_state=0)\nmodel_3 = RandomForestRegressor(n_estimators=100,criterion='mae',random_state=0)\nmodel_4 = RandomForestRegressor(n_estimators=200,min_samples_split=20,random_state=0)\nmodel_5 = RandomForestRegressor(n_estimators=100,max_depth=7,random_state=0)\n\nmodels = [model_1, model_2, model_3, model_4, model_5]","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To select the best model out of the five, we define a function `score_model()` below.  This function returns the mean absolute error (MAE) from the validation set.  Recall that the best model will obtain the lowest MAE.  (_To review **mean absolute error**, look [here](https://www.kaggle.com/dansbecker/model-validation).)_\n\nRun the code cell without changes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Function for comparing different models\ndef score_model(model, X_t=X_train, X_v= X_valid, y_t= y_train, y_v=y_valid):\n    model.fit(X_t, y_t)\n    pred = model.predict(X_v)\n    return mean_absolute_error(y_v, pred)\n\nfor i in range(0,len(models)):\n    mae = score_model(models[i])\n    print(\"Model %d MAE: %d\" %(i+1, mae))","execution_count":5,"outputs":[{"output_type":"stream","text":"Model 1 MAE: 24015\nModel 2 MAE: 23740\nModel 3 MAE: 23528\nModel 4 MAE: 23996\nModel 5 MAE: 23706\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Use the above results to fill in the line below.  Which model is the best model?  Your answer should be one of `model_1`, `model_2`, `model_3`, `model_4`, or `model_5`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the best model\nbest_model = model_3","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Generate test predictions\n\nGreat. You know how to evaluate what makes an accurate model. Now it's time to go through the modeling process and make predictions. In the line below, create a Random Forest model with the variable name `my_model`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a model\nmy_model = RandomForestRegressor() # Your code here\nmy_model = best_model","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell without changes.  The code fits the model to the training and validation data, and then generates test predictions that are saved to a CSV file.  These test predictions can be submitted directly to the competition!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model to the training data\nmy_model.fit(X, y)\n\n# Generate test predictions\npred_test = my_model.predict(X_test)\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': pred_test})\noutput.to_csv('submission.csv', index=False)","execution_count":8,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Submit your results\n\nOnce you have successfully completed Step 2, you're ready to submit your results to the leaderboard!  First, you'll need to join the competition if you haven't already.  So open a new window by clicking on [this link](https://www.kaggle.com/c/home-data-for-ml-course).  Then click on the **Join Competition** button.\n\n![join competition image](https://i.imgur.com/wLmFtH3.png)\n\nNext, follow the instructions below:\n- Begin by clicking on the blue **COMMIT** button in the top right corner of this window.  This will generate a pop-up window.  \n- After your code has finished running, click on the blue **Open Version** button in the top right of the pop-up window.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n- Click on the **Output** tab on the left of the screen.  Then, click on the **Submit to Competition** button to submit your results to the leaderboard.\n- If you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your model and repeat the process."},{"metadata":{},"cell_type":"markdown","source":"# Keep going\n\nYou've made your first model. But how can you quickly make it better?\n\nLearn how to improve your competition results by incorporating columns with **[missing values](https://www.kaggle.com/alexisbcook/missing-values)**.\n\n---\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"},{"metadata":{},"cell_type":"markdown","source":"# [Missing values](https://www.kaggle.com/kernels/fork/3370280)\n\n\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n---\n\nNow it's your turn to test your new knowledge of **missing values** handling. You'll probably find it makes a big difference.\n\n"},{"metadata":{},"cell_type":"markdown","source":"In this exercise, you will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'],inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'],axis=1, inplace=True)\nX_test_full.drop(['SalePrice'],axis=1, inplace=True)\n\n# To keep things simple, we'll use only numerical predictions\nX = X_full.select_dtypes(exclude=['object'])\nX_test = X_test_full.select_dtypes(exclude=['object'])\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the next code cell to print the first five rows of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"     MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\nId                                                                           \n619          20         90.0    11694            9            5       2007   \n871          20         60.0     6600            5            5       1962   \n93           30         80.0    13360            5            7       1921   \n818          20          NaN    13265            8            5       2002   \n303          20        118.0    13704            7            5       2001   \n\n     YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\nId                                                     ...               \n619          2007       452.0          48           0  ...         774   \n871          1962         0.0           0           0  ...         308   \n93           2006         0.0         713           0  ...         432   \n818          2002       148.0        1218           0  ...         857   \n303          2002       150.0           0           0  ...         843   \n\n     WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\nId                                                                              \n619           0          108              0          0          260         0   \n871           0            0              0          0            0         0   \n93            0            0             44          0            0         0   \n818         150           59              0          0            0         0   \n303         468           81              0          0            0         0   \n\n     MiscVal  MoSold  YrSold  \nId                            \n619        0       7    2007  \n871        0       8    2009  \n93         0       8    2009  \n818        0       7    2008  \n303        0       1    2006  \n\n[5 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>619</th>\n      <td>20</td>\n      <td>90.0</td>\n      <td>11694</td>\n      <td>9</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>2007</td>\n      <td>452.0</td>\n      <td>48</td>\n      <td>0</td>\n      <td>...</td>\n      <td>774</td>\n      <td>0</td>\n      <td>108</td>\n      <td>0</td>\n      <td>0</td>\n      <td>260</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>20</td>\n      <td>60.0</td>\n      <td>6600</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1962</td>\n      <td>1962</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>308</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>30</td>\n      <td>80.0</td>\n      <td>13360</td>\n      <td>5</td>\n      <td>7</td>\n      <td>1921</td>\n      <td>2006</td>\n      <td>0.0</td>\n      <td>713</td>\n      <td>0</td>\n      <td>...</td>\n      <td>432</td>\n      <td>0</td>\n      <td>0</td>\n      <td>44</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>818</th>\n      <td>20</td>\n      <td>NaN</td>\n      <td>13265</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2002</td>\n      <td>2002</td>\n      <td>148.0</td>\n      <td>1218</td>\n      <td>0</td>\n      <td>...</td>\n      <td>857</td>\n      <td>150</td>\n      <td>59</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>303</th>\n      <td>20</td>\n      <td>118.0</td>\n      <td>13704</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>150.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>843</td>\n      <td>468</td>\n      <td>81</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2006</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 36 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"You can already see a few missing values in the first several rows.  In the next step, you'll obtain a more comprehensive understanding of the missing values in the dataset.\n\n# Step 1: Preliminary investigation\n\nRun the code cell below without changes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of training data (num_rows, num_columns)\nprint(X_train.shape)\n\n# Number of missing values in each column of traing data\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column>0])","execution_count":11,"outputs":[{"output_type":"stream","text":"(1168, 36)\nLotFrontage    212\nMasVnrArea       6\nGarageYrBlt     58\ndtype: int64\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Part A\n\nUse the above output to answer the questions below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the line below: How many rows are in the training data?\n#num_rows = 1168\nnum_rows, num_columns = X_train.shape\n\n# Fill in the line below: How many columns in the training data\n# have missing values?\n#num_cols_with_missing = 3\nnum_cols_with_missing = len(missing_val_count_by_column[missing_val_count_by_column>0])\n\n# Fill in the line below: How many missing entries are contained in \n# all of the training data?\n#tot_missing = 212+6+58\ntot_missing = missing_val_count_by_column[missing_val_count_by_column>0].sum()\n","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part B\nConsidering your answers above, what do you think is likely the best approach to dealing with the missing values?\n\nHint: Does the dataset have a lot of missing values, or just a few? Would we lose much information if we completely ignored the columns with missing entries?\n\nSolution: Since there are relatively few missing entries in the data (the column with the greatest percentage of missing values is missing less than 20% of its entries), we can expect that dropping columns is unlikely to yield good results. This is because we'd be throwing away a lot of valuable data, and so imputation will likely perform better."},{"metadata":{},"cell_type":"markdown","source":"To compare different approaches to dealing with missing values, you'll use the same `score_dataset()` function from the tutorial.  This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Drop columns with missing values\n\nIn this step, you'll preprocess the data in `X_train` and `X_valid` to remove columns with missing values.  Set the preprocessed DataFrames to `reduced_X_train` and `reduced_X_valid`, respectively.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the line below: get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n\n# Fill in the lines below: drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check: When you've updated the starter code, check() will tell you whether your code is correct. You need to update the code that creates variables reduced_X_train, reduced_X_valid"},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell without changes to obtain the MAE for this approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE (Drop columns with missing values): \")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","execution_count":15,"outputs":[{"output_type":"stream","text":"MAE (Drop columns with missing values): \n17837.82570776256\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Imputation\n\n### Part A\n\nUse the next code cell to impute missing values with the mean value along each column.  Set the preprocessed DataFrames to `imputed_X_train` and `imputed_X_valid`.  Make sure that the column names match those in `X_train` and `X_valid`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Fill in the lines below: imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Fill in the lines below: imputation removed columns names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell without changes to obtain the MAE for this approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))","execution_count":17,"outputs":[{"output_type":"stream","text":"MAE (Imputation):\n18062.894611872147\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Part B\n\nCompare the MAE from each approach.  Does anything surprise you about the results?  Why do you think one approach performed better than the other?\n\nHint: Did removing missing values yield a larger or smaller MAE than imputation? Does this agree with the coding example from the tutorial?\n\nSolution: Given that thre are so few missing values in the dataset, we'd expect imputation to perform better than dropping columns entirely. However, we see that dropping columns performs slightly better! While this can probably partially be attributed to noise in the dataset, another potential explanation is that the imputation method is not a great match to this dataset. That is, maybe instead of filling in the mean value, it makes more sense to set every missing value to a value of 0, to fill in the most frequently encountered value, or to use some other method. For instance, consider the GarageYrBlt column (which indicates the year that the garage was built). It's likely that in some cases, a missing value could indicate a house that does not have a garage. Does it make more sense to fill in the median value along each column in this case? Or could we get better results by filling in the minimum value along each column? It's not quite clear what's best in this case, but perhaps we can rule out some options immediately - for instance, setting missing values in this column to 0 is likely to yield horrible results!"},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Generate test predictions\n\nIn this final step, you'll use any approach of your choosing to deal with missing values.  Once you've preprocessed the training and validation features, you'll train and evaluate a random forest model.  Then, you'll preprocess the test data before generating predictions that can be submitted to the competition!\n\n### Part A\n\nUse the next code cell to preprocess the training and validation data.  Set the preprocessed DataFrames to `final_X_train` and `final_X_valid`.  **You can use any approach of your choosing here!**  in order for this step to be marked as correct, you need only ensure:\n- the preprocessed DataFrames have the same number of columns,\n- the preprocessed DataFrames have no missing values, \n- `final_X_train` and `y_train` have the same number of rows, and\n- `final_X_valid` and `y_valid` have the same number of rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"median_imputer = SimpleImputer(strategy='median')\n\nmedian_imputed_X_train = pd.DataFrame(median_imputer.fit_transform(X_train))\nmedian_imputed_X_valid = pd.DataFrame(median_imputer.transform(X_valid))\n\nmedian_imputed_X_train.columns = X_train.columns\nmedian_imputed_X_valid.columns = X_valid.columns\n\nprint(\"MAE (Median Imputation):\")\nprint(score_dataset(median_imputed_X_train, median_imputed_X_valid, y_train, y_valid))","execution_count":18,"outputs":[{"output_type":"stream","text":"MAE (Median Imputation):\n17791.59899543379\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_frequent_imputer = SimpleImputer(strategy = 'most_frequent')\n\nmost_frequent_imputed_X_train = pd.DataFrame(most_frequent_imputer.fit_transform(X_train))\nmost_frequent_imputed_X_valid = pd.DataFrame(most_frequent_imputer.transform(X_valid))\n\nmost_frequent_imputed_X_train.columns = X_train.columns\nmost_frequent_imputed_X_valid.columns = X_valid.columns\n\nprint(\"MAE (most_frequent Imputation):\")\nprint(score_dataset(most_frequent_imputed_X_train, most_frequent_imputed_X_valid, y_train, y_valid))","execution_count":19,"outputs":[{"output_type":"stream","text":"MAE (most_frequent Imputation):\n17956.065479452056\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessed training and validation features\nfinal_imputer = median_imputer\nfinal_X_train = median_imputed_X_train\nfinal_X_valid = median_imputed_X_valid\n","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell to train and evaluate a random forest model.  (*Note that we don't use the `score_dataset()` function above, because we will soon use the trained model to generate test predictions!*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define and fit the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel.fit(final_X_train, y_train)\n\n# Get validation predictions and MAE\npreds_valid = model.predict(final_X_valid)\n\nprint(\"MAE (Your approach): \")\nprint(mean_absolute_error(y_valid, preds_valid))","execution_count":21,"outputs":[{"output_type":"stream","text":"MAE (Your approach): \n17791.59899543379\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Part B\n\nUse the next code cell to preprocess your test data.  Make sure that you use a method that agrees with how you preprocessed the training and validation data, and set the preprocessed test features to `final_X_test`.\n\nThen, use the preprocessed test features and the trained model to generate test predictions in `preds_test`.\n\nIn order for this step to be marked correct, you need only ensure:\n- the preprocessed test DataFrame has no missing values, and\n- `final_X_test` has the same number of rows as `X_test`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the line below: preprocess test data\nfinal_X_test = pd.DataFrame(final_imputer.fit_transform(X_test))\n                            \n# Fill in the line below: get test predictions\npreds_test = model.predict(final_X_test)\n","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell without changes to save your results to a CSV file that can be submitted directly to the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save test predictions to file\noutput = pd.DataFrame({'Id':X_test.index,\n                       'SalePrice':preds_test})\noutput.to_csv('submission.csv', index=False)","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Submit your results\n\nOnce you have successfully completed Step 4, you're ready to submit your results to the leaderboard!  (_You also learned how to do this in the previous exercise.  If you need a reminder of how to do this, please use the instructions below._)  \n\nFirst, you'll need to join the competition if you haven't already.  So open a new window by clicking on [this link](https://www.kaggle.com/c/home-data-for-ml-course).  Then click on the **Join Competition** button.\n\n![join competition image](https://i.imgur.com/wLmFtH3.png)\n\nNext, follow the instructions below:\n- Begin by clicking on the blue **COMMIT** button in the top right corner.  This will generate a pop-up window.  \n- After your code has finished running, click on the blue **Open Version** button in the top right of the pop-up window.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n- Click on the **Output** tab on the left of the screen.  Then, click on the **Submit to Competition** button to submit your results to the leaderboard.\n- If you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your model and repeat the process.\n\n# Keep going\n\nMove on to learn what **[categorical variables](https://www.kaggle.com/alexisbcook/categorical-variables)** are, along with how to incorporate them into your machine learning models.  Categorical variables are very common in real-world data, but you'll get an error if you try to plug them into your models without processing them first!"},{"metadata":{},"cell_type":"markdown","source":"---\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"},{"metadata":{},"cell_type":"markdown","source":"# [Categorical Variables](https://www.kaggle.com/kernels/fork/3370279)\n\n\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n---\n\nBy encoding **categorical variables**, you'll obtain your best results thus far!\n"},{"metadata":{},"cell_type":"markdown","source":"In this exercise, you will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\nX_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()]\nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validatin set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the next code cell to print the first five rows of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"     MSSubClass MSZoning  LotArea Street LotShape LandContour Utilities  \\\nId                                                                        \n619          20       RL    11694   Pave      Reg         Lvl    AllPub   \n871          20       RL     6600   Pave      Reg         Lvl    AllPub   \n93           30       RL    13360   Pave      IR1         HLS    AllPub   \n818          20       RL    13265   Pave      IR1         Lvl    AllPub   \n303          20       RL    13704   Pave      IR1         Lvl    AllPub   \n\n    LotConfig LandSlope Neighborhood  ... OpenPorchSF EnclosedPorch 3SsnPorch  \\\nId                                    ...                                       \n619    Inside       Gtl      NridgHt  ...         108             0         0   \n871    Inside       Gtl        NAmes  ...           0             0         0   \n93     Inside       Gtl      Crawfor  ...           0            44         0   \n818   CulDSac       Gtl      Mitchel  ...          59             0         0   \n303    Corner       Gtl      CollgCr  ...          81             0         0   \n\n    ScreenPorch  PoolArea  MiscVal  MoSold  YrSold SaleType SaleCondition  \nId                                                                         \n619         260         0        0       7    2007      New       Partial  \n871           0         0        0       8    2009       WD        Normal  \n93            0         0        0       8    2009       WD        Normal  \n818           0         0        0       7    2008       WD        Normal  \n303           0         0        0       1    2006       WD        Normal  \n\n[5 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>LandSlope</th>\n      <th>Neighborhood</th>\n      <th>...</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>619</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>11694</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>NridgHt</td>\n      <td>...</td>\n      <td>108</td>\n      <td>0</td>\n      <td>0</td>\n      <td>260</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2007</td>\n      <td>New</td>\n      <td>Partial</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>6600</td>\n      <td>Pave</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>NAmes</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2009</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>30</td>\n      <td>RL</td>\n      <td>13360</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>HLS</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>Crawfor</td>\n      <td>...</td>\n      <td>0</td>\n      <td>44</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2009</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>818</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>13265</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>CulDSac</td>\n      <td>Gtl</td>\n      <td>Mitchel</td>\n      <td>...</td>\n      <td>59</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>303</th>\n      <td>20</td>\n      <td>RL</td>\n      <td>13704</td>\n      <td>Pave</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>CollgCr</td>\n      <td>...</td>\n      <td>81</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Notice that the dataset contains both numerical and categorical variables.  You'll need to encode the categorical data before training a model.\n\nTo compare different models, you'll use the same `score_dataset()` function from the tutorial.  This function reports the [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) from a random forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Drop columns with categorical data\n\nYou'll get started with the most straightforward approach.  Use the code cell below to preprocess the data in `X_train` and `X_valid` to remove columns with categorical data.  Set the preprocessed DataFrames to `drop_X_train` and `drop_X_valid`, respectively.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exclude categorical variables (data type: object) from the dataset\n# Drop columns in training and validation data\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE from Approach 1 (Drop categorical variables): \")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","execution_count":28,"outputs":[{"output_type":"stream","text":"MAE from Approach 1 (Drop categorical variables): \n17837.82570776256\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Label encoding\n\nBefore jumping into label encoding, we'll investigate the dataset.  Specifically, we'll look at the `'Condition2'` column.  The code cell below prints the unique entries in both the training and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique values in 'Condition2' column in training data: \", X_train['Condition2'].unique())\nprint(\"\\n Unique values in 'Condition2' column in validation data: \", X_valid['Condition2'].unique())","execution_count":29,"outputs":[{"output_type":"stream","text":"Unique values in 'Condition2' column in training data:  ['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n\n Unique values in 'Condition2' column in validation data:  ['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"If you now write code to: \n- fit a label encoder to the training data, and then \n- use it to transform both the training and validation data, \n\nyou'll get an error.  Can you see why this is the case?  (_You'll need  to use the above output to answer this question._)\n\nHint: Are there any values that appear in the validation data but not in the training data?\n\nSolution: Fitting a label encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them. Notice that the 'Condition2' column in the validation data contains the values 'RRAn' and 'RRNn', but these don't appear in the training data -- thus, if we try to use a label encoder with scikit-learn, the code will throw an error."},{"metadata":{},"cell_type":"markdown","source":"This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue.  For instance, you can write a custom label encoder to deal with new categories.  The simplest approach, however, is to drop the problematic categorical columns.  \n\nRun the code cell below to save the problematic columns to a Python list `bad_label_cols`.  Likewise, columns that can be safely label encoded are stored in `good_label_cols`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# All categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype==\"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if set(X_train[col])==set(X_valid[col])]\n\n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\nprint(\"Categorical columns that will be label encoded: \",good_label_cols)\nprint(\"\\n Categorical columns that will be dropped from dataset: \",bad_label_cols)\n\n","execution_count":30,"outputs":[{"output_type":"stream","text":"Categorical columns that will be label encoded:  ['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'BldgType', 'HouseStyle', 'ExterQual', 'CentralAir', 'KitchenQual', 'PavedDrive', 'SaleCondition']\n\n Categorical columns that will be dropped from dataset:  ['Exterior2nd', 'Utilities', 'Functional', 'Neighborhood', 'RoofStyle', 'SaleType', 'Exterior1st', 'ExterCond', 'LandSlope', 'HeatingQC', 'Condition2', 'Condition1', 'Heating', 'RoofMatl', 'Foundation']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Use the next code cell to label encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `label_X_train` and `label_X_valid`, respectively.  \n- We have provided code below to drop the categorical columns in `bad_label_cols` from the dataset. \n- You should label encode the categorical columns in `good_label_cols`.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply label encoder\nlabel_encoder = LabelEncoder()\n\nfor col in good_label_cols:\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE from Approach 2 (Label Encoding): \")\nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","execution_count":32,"outputs":[{"output_type":"stream","text":"MAE from Approach 2 (Label Encoding): \n17575.291883561644\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Investigating cardinality\n\nSo far, you've tried two different approaches to dealing with categorical variables.  And, you've seen that encoding categorical data yields better results than removing columns from the dataset.\n\nSoon, you'll try one-hot encoding.  Before then, there's one additional topic we need to cover.  Begin by running the next code cell without changes.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get number of unique entries in each column with categorical data\nobject_unique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_unique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"[('Street', 2),\n ('Utilities', 2),\n ('CentralAir', 2),\n ('LandSlope', 3),\n ('PavedDrive', 3),\n ('LotShape', 4),\n ('LandContour', 4),\n ('ExterQual', 4),\n ('KitchenQual', 4),\n ('MSZoning', 5),\n ('LotConfig', 5),\n ('BldgType', 5),\n ('ExterCond', 5),\n ('HeatingQC', 5),\n ('Condition2', 6),\n ('RoofStyle', 6),\n ('Foundation', 6),\n ('Heating', 6),\n ('Functional', 6),\n ('SaleCondition', 6),\n ('RoofMatl', 7),\n ('HouseStyle', 8),\n ('Condition1', 9),\n ('SaleType', 9),\n ('Exterior1st', 15),\n ('Exterior2nd', 16),\n ('Neighborhood', 25)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The output above shows, for each column with categorical data, the number of unique values in the column.  For instance, the `'Street'` column in the training data has two unique values: `'Grvl'` and `'Pave'`, corresponding to a gravel road and a paved road, respectively.\n\nWe refer to the number of unique entries of a categorical variable as the **cardinality** of that categorical variable.  For instance, the `'Street'` variable has cardinality 2.\n\nUse the output above to answer the questions below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the line below: How many categorical variables in the training data\n# have cardinality greater than 10?\nhigh_cardinality_numcols = 3\n\n# Fill in the line below: How many columns are needed to one-hot encode the \n# 'Neighborhood' variable in the training data?\nnum_cols_neighborhood = 25\nnum_cols_neighborhood = d['Neighborhood']\n","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use label encoding.\n\nAs an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.  \n- If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?  \n- If we instead replace the column with the label encoding, how many entries are added?  \n\nUse your answers to fill in the lines below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the line below: How many entries are added to the dataset by \n# replacing the column with a one-hot encoding?\nOH_entries_added = 10000*100-10000\n\n# Fill in the line below: How many entries are added to the dataset by\n# replacing the column with a label encoding?\nlabel_entries_added = 0\n","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution:\n\n# How many entries are added to the dataset by\n# replacing the column with a one-hot encoding?\nOH_entries_added = 1e4*100 - 1e4\n\n# How many entries are added to the dataset by\n# replacing the column with a label encoding?\nlabel_entries_added = 0","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: One-hot encoding\n\nIn this step, you'll experiment with one-hot encoding.  But, instead of encoding all of the categorical variables in the dataset, you'll only create a one-hot encoding for columns with cardinality less than 10.\n\nRun the code cell below without changes to set `low_cardinality_cols` to a Python list containing the columns that will be one-hot encoded.  Likewise, `high_cardinality_cols` contains a list of categorical columns that will be dropped from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint(\"Categorical columns that will be one-hot encoded: \",low_cardinality_cols)\nprint(\"\\n Categorical columns that will be dropped from the dataset: \", high_cardinality_cols)","execution_count":37,"outputs":[{"output_type":"stream","text":"Categorical columns that will be one-hot encoded:  ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']\n\n Categorical columns that will be dropped from the dataset:  ['Exterior1st', 'Exterior2nd', 'Neighborhood']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Use the next code cell to one-hot encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `OH_X_train` and `OH_X_valid`, respectively.  \n- The full list of categorical columns in the dataset can be found in the Python list `object_cols`.\n- You should only one-hot encode the categorical columns in `low_cardinality_cols`.  All other categorical columns should be dropped from the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell to get the MAE for this approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","execution_count":39,"outputs":[{"output_type":"stream","text":"MAE from Approach 3 (One-Hot Encoding):\n17525.345719178084\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 5: Generate test predictions and submit your results\n\nAfter you complete Step 4, if you'd like to use what you've learned to submit your results to the leaderboard, you'll need to preprocess the test data before generating predictions.\n\n**This step is completely optional, and you do not need to submit results to the leaderboard to successfully complete the exercise.**\n\nCheck out the previous exercise if you need help with remembering how to [join the competition](https://www.kaggle.com/c/home-data-for-ml-course) or save your results to CSV.  Once you have generated a file with your results, follow the instructions below:\n- Begin by clicking on the blue **COMMIT** button in the top right corner.  This will generate a pop-up window.  \n- After your code has finished running, click on the blue **Open Version** button in the top right of the pop-up window.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n- Click on the **Output** tab on the left of the screen.  Then, click on the **Submit to Competition** button to submit your results to the leaderboard.\n- If you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your model and repeat the process."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keep going\n\nWith missing value handling and categorical encoding, your modeling process is getting complex. This complexity gets worse when you want to save your model to use in the future. The key to managing this complexity is something called **pipelines**. \n\n**[Learn to use pipelines](https://www.kaggle.com/alexisbcook/pipelines)** to preprocess datasets with categorical variables, missing values and any other messiness your data throws at you.\n\n---\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"},{"metadata":{},"cell_type":"markdown","source":"# [Pipelines](https://www.kaggle.com/kernels/fork/3370278)\n\n\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n---\n\n\nIn this exercise, you will use **pipelines** to improve the efficiency of your machine learning code.\n\n"},{"metadata":{},"cell_type":"markdown","source":"You will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, train_size=0.8, random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_col = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\n\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64','float64']]\n\n# Keep selected columns only\nmy_cols = categorical_col + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"    MSZoning Street Alley LotShape LandContour Utilities LotConfig LandSlope  \\\nId                                                                             \n619       RL   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n871       RL   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl   \n93        RL   Pave  Grvl      IR1         HLS    AllPub    Inside       Gtl   \n818       RL   Pave   NaN      IR1         Lvl    AllPub   CulDSac       Gtl   \n303       RL   Pave   NaN      IR1         Lvl    AllPub    Corner       Gtl   \n\n    Condition1 Condition2  ... GarageArea WoodDeckSF OpenPorchSF  \\\nId                         ...                                     \n619       Norm       Norm  ...        774          0         108   \n871       PosN       Norm  ...        308          0           0   \n93        Norm       Norm  ...        432          0           0   \n818       Norm       Norm  ...        857        150          59   \n303       Norm       Norm  ...        843        468          81   \n\n    EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold  \nId                                                                      \n619             0         0         260        0       0      7   2007  \n871             0         0           0        0       0      8   2009  \n93             44         0           0        0       0      8   2009  \n818             0         0           0        0       0      7   2008  \n303             0         0           0        0       0      1   2006  \n\n[5 rows x 76 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSZoning</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>LandSlope</th>\n      <th>Condition1</th>\n      <th>Condition2</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>619</th>\n      <td>RL</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>774</td>\n      <td>0</td>\n      <td>108</td>\n      <td>0</td>\n      <td>0</td>\n      <td>260</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>RL</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>PosN</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>308</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>RL</td>\n      <td>Pave</td>\n      <td>Grvl</td>\n      <td>IR1</td>\n      <td>HLS</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>432</td>\n      <td>0</td>\n      <td>0</td>\n      <td>44</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>818</th>\n      <td>RL</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>CulDSac</td>\n      <td>Gtl</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>857</td>\n      <td>150</td>\n      <td>59</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>303</th>\n      <td>RL</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>843</td>\n      <td>468</td>\n      <td>81</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2006</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 76 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps = [\n    ('imputer',SimpleImputer(strategy='most_frequent')),\n    ('onehot',OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(transformers=[\n    ('num',numerical_transformer,numerical_cols),\n    ('cat',categorical_transformer,categorical_col)\n])\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a a pipeline\nclf = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model)\n])\n\n# Preprocessing of traininf data, fit model\nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n\nprint(\"MAE: \",mean_absolute_error(y_valid, preds))\n","execution_count":42,"outputs":[{"output_type":"stream","text":"MAE:  17861.780102739725\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The code yields a value around 17862 for the mean absolute error (MAE).  In the next step, you will amend the code to do better.\n\n# Step 1: Improve the performance\n\n### Part A\n\nNow, it's your turn!  In the code cell below, define your own preprocessing steps and random forest model.  Fill in values for the following variables:\n- `numerical_transformer`\n- `categorical_transformer`\n- `model`\n\nTo pass this part of the exercise, you need only define valid preprocessing steps and a random forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer',SimpleImputer(strategy='constant')),\n    ('onehot',OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(transformers=[\n    ('num',numerical_transformer,numerical_cols),\n    ('cat',categorical_transformer,categorical_col)\n])\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#Solution:\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\"\"\"","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"\"\\n#Solution:\\n\\n# Preprocessing for numerical data\\nnumerical_transformer = SimpleImputer(strategy='constant')\\n\\n# Preprocessing for categorical data\\ncategorical_transformer = Pipeline(steps=[\\n    ('imputer', SimpleImputer(strategy='constant')),\\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\\n])\\n\\n# Bundle preprocessing for numerical and categorical data\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        ('num', numerical_transformer, numerical_cols),\\n        ('cat', categorical_transformer, categorical_cols)\\n    ])\\n\\n# Define model\\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\\n\""},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Part B\n\nRun the code cell below without changes.\n\nTo pass this step, you need to have defined a pipeline in **Part A** that achieves lower MAE than the code above.  You're encouraged to take your time here and try out many different approaches, to see how low you can get the MAE!  (_If your code does not pass, please amend the preprocessing steps and model in Part A._)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[\n    ('preprocessor',preprocessor),\n    ('model',model)\n])\n\n# Preprocessing of training data, fit model\nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint(\"MAE: \",score)","execution_count":45,"outputs":[{"output_type":"stream","text":"MAE:  17487.872363013696\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Generate test predictions\n\nNow, you'll use your trained model to generate predictions with the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing of test data, fit model\npreds_test = my_pipeline.predict(X_test) # Your code here\n","execution_count":46,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the next code cell without changes to save your results to a CSV file that can be submitted directly to the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})      \noutput.to_csv('submission.csv', index=False)     ","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Submit your results\n\nOnce you have successfully completed Step 2, you're ready to submit your results to the leaderboard!  If you choose to do so, make sure that you have already joined the competition by clicking on the **Join Competition** button at [this link](https://www.kaggle.com/c/home-data-for-ml-course).  \n- Begin by clicking on the blue **COMMIT** button in the top right corner.  This will generate a pop-up window.  \n- After your code has finished running, click on the blue **Open Version** button in the top right of the pop-up window.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n- Click on the **Output** tab on the left of the screen.  Then, click on the **Submit to Competition** button to submit your results to the leaderboard.\n- If you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your model and repeat the process.\n\n# Keep going\n\nMove on to learn about [**cross-validation**](https://www.kaggle.com/alexisbcook/cross-validation), a technique you can use to obtain more accurate estimates of model performance!\n\n\n---\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"},{"metadata":{},"cell_type":"markdown","source":"# [Cross-Validation](https://www.kaggle.com/kernels/fork/3370281)\n\n\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n---\n\n\nIn this exercise, you will leverage what you've learned to tune a machine learning model with **cross-validation**.\n\nYou will work with the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) from the previous exercise. \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`.\n\nFor simplicity, we drop categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Read the data\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = train_data.SalePrice\ntrain_data.drop(['SalePrice'], axis=1, inplace=True)\n\n# Select numeric columns only\nnumeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64','float64']]\nX = train_data[numeric_cols].copy()\nX_test = test_data[numeric_cols].copy()\n","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the next code cell to print the first several rows of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"    MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\nId                                                                          \n1           60         65.0     8450            7            5       2003   \n2           20         80.0     9600            6            8       1976   \n3           60         68.0    11250            7            5       2001   \n4           70         60.0     9550            7            5       1915   \n5           60         84.0    14260            8            5       2000   \n\n    YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\nId                                                    ...               \n1           2003       196.0         706           0  ...         548   \n2           1976         0.0         978           0  ...         460   \n3           2002       162.0         486           0  ...         608   \n4           1970         0.0         216           0  ...         642   \n5           2000       350.0         655           0  ...         836   \n\n    WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\nId                                                                             \n1            0           61              0          0            0         0   \n2          298            0              0          0            0         0   \n3            0           42              0          0            0         0   \n4            0           35            272          0            0         0   \n5          192           84              0          0            0         0   \n\n    MiscVal  MoSold  YrSold  \nId                           \n1         0       2    2008  \n2         0       5    2007  \n3         0       9    2008  \n4         0       2    2006  \n5         0      12    2008  \n\n[5 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n    <tr>\n      <th>Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>60</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>196.0</td>\n      <td>706</td>\n      <td>0</td>\n      <td>...</td>\n      <td>548</td>\n      <td>0</td>\n      <td>61</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>6</td>\n      <td>8</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>0.0</td>\n      <td>978</td>\n      <td>0</td>\n      <td>...</td>\n      <td>460</td>\n      <td>298</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>162.0</td>\n      <td>486</td>\n      <td>0</td>\n      <td>...</td>\n      <td>608</td>\n      <td>0</td>\n      <td>42</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>70</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>0.0</td>\n      <td>216</td>\n      <td>0</td>\n      <td>...</td>\n      <td>642</td>\n      <td>0</td>\n      <td>35</td>\n      <td>272</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>60</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>350.0</td>\n      <td>655</td>\n      <td>0</td>\n      <td>...</td>\n      <td>836</td>\n      <td>192</td>\n      <td>84</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 36 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"So far, you've learned how to build pipelines with scikit-learn.  For instance, the pipeline below will use [`SimpleImputer()`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) to replace missing values in the data, before using [`RandomForestRegressor()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to train a random forest model to make predictions.  We set the number of trees in the random forest model with the `n_estimators` parameter, and setting `random_state` ensures reproducibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nmy_pipeline = Pipeline(steps=[\n    ('preprocesor',SimpleImputer()),\n    ('model',RandomForestRegressor(n_estimators=50, random_state=0))\n])","execution_count":50,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You have also learned how to use pipelines in cross-validation.  The code below uses the [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to obtain the mean absolute error (MAE), averaged across five different folds.  Recall we set the number of folds with the `cv` parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n\nprint(\"Average MAE score: \", scores.mean())","execution_count":51,"outputs":[{"output_type":"stream","text":"Average MAE score:  18276.410356164386\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Write a useful function\n\nIn this exercise, you'll use cross-validation to select parameters for a machine learning model.\n\nBegin by writing a function `get_score()` that reports the average (over three cross-validation folds) MAE of a machine learning pipeline that uses:\n- the data in `X` and `y` to create folds,\n- `SimpleImputer()` (with all parameters left as default) to replace missing values, and\n- `RandomForestRegressor()` (with `random_state=0`) to fit a random forest model.\n\nThe `n_estimators` parameter supplied to `get_score()` is used when setting the number of trees in the random forest model.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_score(n_estimators):\n    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n    \n    Keyword argument:\n    n_estimators -- the number of trees in the forest\n    \"\"\"\n    # Replace this body with your own code\n    my_pipeline = Pipeline(steps=[\n        ('preprocessor', SimpleImputer()),\n        ('model', RandomForestRegressor(n_estimators=n_estimators, random_state=0))\n    ])\n    \n    scores = -1 * cross_val_score(my_pipeline, X, y, cv=3, scoring='neg_mean_absolute_error')\n    \n    return scores.mean()","execution_count":52,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Test different parameter values\n\nNow, you will use the function that you defined in Step 1 to evaluate the model performance corresponding to eight different values for the number of trees in the random forest: 50, 100, 150, ..., 300, 350, 400.\n\nStore your results in a Python dictionary `results`, where `results[i]` is the average MAE returned by `get_scores(i)`."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50,450,50):\n    print(i)\n    ","execution_count":53,"outputs":[{"output_type":"stream","text":"50\n100\n150\n200\n250\n300\n350\n400\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = {}\n\nfor i in range(50,450,50):\n    results[i] = get_score(i)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nresults = {} # Your code here\n\nfor i in range(1,9):\n    results[i*50] = get_score(i*50)\n\"\"\"","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"'\\nresults = {} # Your code here\\n\\nfor i in range(1,9):\\n    results[i*50] = get_score(i*50)\\n'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Find the best parameter value\n\nUse the next cell to visualize your results from Step 2.  Run the code without changes."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(results.keys(), results.values())\nplt.show()","execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"Given the results, which value for `n_estimators` seems best for the random forest model?  Use your answer to set the value of `n_estimators_best`."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators_best = 200","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this exercise, you have explored one method for choosing appropriate parameters in a machine learning model.  \n\nIf you'd like to learn more about [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization), you're encouraged to start with **grid search**, which is a straightforward method for determining the best _combination_ of parameters for a machine learning model.  Thankfully, scikit-learn also contains a built-in function [`GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) that can make your grid search code very efficient!\n\n# Keep going\n\nContinue to learn about **[gradient boosting](https://www.kaggle.com/alexisbcook/xgboost)**, a powerful technique that achieves state-of-the-art results on a variety of datasets.\n\n\n\n---\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"},{"metadata":{},"cell_type":"markdown","source":"# [XGBoost](https://www.kaggle.com/kernels/fork/3370271)\n\n\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n---\n\n\n\nIn this exercise, you will use your new knowledge to train a model with **gradient boosting**.\n"},{"metadata":{},"cell_type":"markdown","source":"You will work with the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) dataset from the previous exercise. \n\n![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\nX_test_full = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\n\n# Remove rows with missing target\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from trainig data\nX_train_full,X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8,test_size=0.2, random_state=0)\n\n# \"Cardinality\" means the numer of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64','float64']]\n\n# Keep selected columns only\nmy_cols = low_categorical_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","execution_count":58,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Build model\n\nIn this step, you'll build and train your first model with gradient boosting.\n\n- Begin by setting `my_model_1` to an XGBoost model.  Use the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class, and set the random seed to 0 (`random_state=0`).  **Leave all other parameters as default.**\n- Then, fit the model to the training data in `X_train` and `y_train`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# Define the model\nmy_model_1 = XGBRegressor(random_state=0)\n\n# Fit the model\nmy_model_1.fit(X_train, y_train)","execution_count":59,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n  if getattr(data, 'base', None) is not None and \\\n","name":"stderr"},{"output_type":"stream","text":"[15:25:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","name":"stdout"},{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0,\n             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Set `predictions_1` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\n# Get predictions\npredictions_1 = my_model_1.predict(X_valid)","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions for the validation set.  Recall that the labels for the validation data are stored in `y_valid`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate MAE\nmae_1 = mean_absolute_error(predictions_1, y_valid)\n\n# Print MAE\nprint(\"Mean Absolute Error: \", mae_1)","execution_count":61,"outputs":[{"output_type":"stream","text":"Mean Absolute Error:  16803.434690710616\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: Improve the model\n\nNow that you've trained a default model as baseline, it's time to tinker with the parameters, to see if you can get better performance!\n- Begin by setting `my_model_2` to an XGBoost model, using the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class.  Use what you learned in the previous tutorial to figure out how to change the default parameters (like `n_estimators` and `learning_rate`) to get better results.\n- Then, fit the model to the training data in `X_train` and `y_train`.\n- Set `predictions_2` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`.\n- Finally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions on the validation set.  Recall that the labels for the validation data are stored in `y_valid`.\n\nIn order for this step to be marked correct, your model in `my_model_2` must attain lower MAE than the model in `my_model_1`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the model\nmy_model_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n\n# Fit the model\nmy_model_2.fit(X_train, y_train)\n\n# Get predictions\npredictions_2 = my_model_2.predict(X_valid)\n\n# Calculate MAE\nmae_2 = mean_absolute_error(predictions_2, y_valid)\n\n# Print MAE\nprint(\"Mean Absolute Error: \", mae_2)","execution_count":62,"outputs":[{"output_type":"stream","text":"[15:25:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n  if getattr(data, 'base', None) is not None and \\\n","name":"stderr"},{"output_type":"stream","text":"Mean Absolute Error:  16084.123354559075\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Break the model\n\nIn this step, you will create a model that performs worse than the original model in Step 1.  This will help you to develop your intuition for how to set parameters.  You might even find that you accidentally get better performance, which is ultimately a nice problem to have and a valuable learning experience!\n- Begin by setting `my_model_3` to an XGBoost model, using the [XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor) class.  Use what you learned in the previous tutorial to figure out how to change the default parameters (like `n_estimators` and `learning_rate`) to design a model to get high MAE.\n- Then, fit the model to the training data in `X_train` and `y_train`.\n- Set `predictions_3` to the model's predictions for the validation data.  Recall that the validation features are stored in `X_valid`.\n- Finally, use the `mean_absolute_error()` function to calculate the mean absolute error (MAE) corresponding to the predictions on the validation set.  Recall that the labels for the validation data are stored in `y_valid`.\n\nIn order for this step to be marked correct, your model in `my_model_3` must attain higher MAE than the model in `my_model_1`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the model\nmy_model_3 = XGBRegressor(n_estimators=50, learning_rate=0.05)\n\n# Fit the model\nmy_model_3.fit(X_train, y_train)\n\n# Get the predictions\npredictions_3 = my_model_3.predict(X_valid)\n\n# Calculate MAE\nmae_3 = mean_absolute_error(predictions_3, y_valid)\n\n\n# Print MAE\nprint(\"Mean Absolute Error: \",mae_3)","execution_count":63,"outputs":[{"output_type":"stream","text":"[15:25:58] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n  if getattr(data, 'base', None) is not None and \\\n","name":"stderr"},{"output_type":"stream","text":"Mean Absolute Error:  23222.248956549658\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Keep going\n\nContinue to learn about **[data leakage](https://www.kaggle.com/alexisbcook/data-leakage)**.  This is an important issue for a data scientist to understand, and it has the potential to ruin your models in subtle and dangerous ways!\n\n\n\n---\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*\n\n"},{"metadata":{},"cell_type":"markdown","source":"# [Data Leakage](https://www.kaggle.com/kernels/fork/3370270)\n\n\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n---\n\n\n\nMost people find target leakage very tricky until they've thought about it for a long time.\n\nSo, before trying to think about leakage in the housing price example, we'll go through a few examples in other applications. Things will feel more familiar once you come back to a question about house prices.\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. The Data Science of Shoelaces\n\nNike has hired you as a data science consultant to help them save money on shoe materials. Your first assignment is to review a model one of their employees built to predict how many shoelaces they'll need each month. The features going into the machine learning model include:\n- The current month (January, February, etc)\n- Advertising expenditures in the previous month\n- Various macroeconomic features (like the unemployment rate) as of the beginning of the current month\n- The amount of leather they ended up using in the current month\n\nThe results show the model is almost perfectly accurate if you include the feature about how much leather they used. But it is only moderately accurate if you leave that feature out. You realize this is because the amount of leather they use is a perfect indicator of how many shoes they produce, which in turn tells you how many shoelaces they need.\n\nDo you think the _leather used_ feature constitutes a source of data leakage? If your answer is \"it depends,\" what does it depend on?\n\nAfter you have thought about your answer, check it against the \n\nSolution: This is tricky, and it depends on details of how data is collected (which is common when thinking about leakage). Would you at the beginning of the month decide how much leather will be used that month? If so, this is ok. But if that is determined during the month, you would not have access to it when you make the prediction. If you have a guess at the beginning of the month, and it is subsequently changed during the month, the actual amount used during the month cannot be used as a feature (because it causes leakage).solution below."},{"metadata":{},"cell_type":"markdown","source":"# 2. Return of the Shoelaces\n\nYou have a new idea. You could use the amount of leather Nike ordered (rather than the amount they actually used) leading up to a given month as a predictor in your shoelace model.\n\nDoes this change your answer about whether there is a leakage problem? If you answer \"it depends,\" what does it depend on?\n\n\nSolution: This could be fine, but it depends on whether they order shoelaces first or leather first. If they order shoelaces first, you won't know how much leather they've ordered when you predict their shoelace needs. If they order leather first, then you'll have that number available when you place your shoelace order, and you should be ok."},{"metadata":{},"cell_type":"markdown","source":"# 3. Getting Rich With Cryptocurrencies?\n\nYou saved Nike so much money that they gave you a bonus. Congratulations.\n\nYour friend, who is also a data scientist, says he has built a model that will let you turn your bonus into millions of dollars. Specifically, his model predicts the price of a new cryptocurrency (like Bitcoin, but a newer one) one day ahead of the moment of prediction. His plan is to purchase the cryptocurrency whenever the model says the price of the currency (in dollars) is about to go up.\n\nThe most important features in his model are:\n- Current price of the currency\n- Amount of the currency sold in the last 24 hours\n- Change in the currency price in the last 24 hours\n- Change in the currency price in the last 1 hour\n- Number of new tweets in the last 24 hours that mention the currency\n\nThe value of the cryptocurrency in dollars has fluctuated up and down by over \\$100 in the last year, and yet his model's average error is less than \\$1. He says this is proof his model is accurate, and you should invest with him, buying the currency whenever the model says it is about to go up.\n\nIs he right? If there is a problem with his model, what is it?\n\n\nSolution: There is no source of leakage here. These features should be available at the moment you want to make a predition, and they're unlikely to be changed in the training data after the prediction target is determined. But, the way he describes accuracy could be misleading if you aren't careful. If the price moves gradually, today's price will be an accurate predictor of tomorrow's price, but it may not tell you whether it's a good time to invest. For instance, if it is  100today,amodelpredictingapriceof100today,amodelpredictingapriceof 100 tomorrow may seem accurate, even if it can't tell you whether the price is going up or down from the current price. A better prediction target would be the change in price over the next day. If you can consistently predict whether the price is about to go up or down (and by how much), you may have a winning investment opportunity."},{"metadata":{},"cell_type":"markdown","source":"# 4. Preventing Infections\n\nAn agency that provides healthcare wants to predict which patients from a rare surgery are at risk of infection, so it can alert the nurses to be especially careful when following up with those patients.\n\nYou want to build a model. Each row in the modeling dataset will be a single patient who received the surgery, and the prediction target will be whether they got an infection.\n\nSome surgeons may do the procedure in a manner that raises or lowers the risk of infection. But how can you best incorporate the surgeon information into the model?\n\nYou have a clever idea. \n1. Take all surgeries by each surgeon and calculate the infection rate among those surgeons.\n2. For each patient in the data, find out who the surgeon was and plug in that surgeon's average infection rate as a feature.\n\nDoes this pose any target leakage issues?\nDoes it pose any train-test contamination issues?\n\n\nSolution: This poses a risk of both target leakage and train-test contamination (though you may be able to avoid both if you are careful).\n\nYou have target leakage if a given patient's outcome contributes to the infection rate for his surgeon, which is then plugged back into the prediction model for whether that patient becomes infected. You can avoid target leakage if you calculate the surgeon's infection rate by using only the surgeries before the patient we are predicting for. Calculating this for each surgery in your training data may be a little tricky.\n\nYou also have a train-test contamination problem if you calculate this using all surgeries a surgeon performed, including those from the test-set. The result would be that your model could look very accurate on the test set, even if it wouldn't generalize well to new patients after the model is deployed. This would happen because the surgeon-risk feature accounts for data in the test set. Test sets exist to estimate how the model will do when seeing new data. So this contamination defeats the purpose of the test set.\n\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 5. Housing Prices\n\nYou will build a model to predict housing prices.  The model will be deployed on an ongoing basis, to predict the price of a new house when a description is added to a website.  Here are four features that could be used as predictors.\n1. Size of the house (in square meters)\n2. Average sales price of homes in the same neighborhood\n3. Latitude and longitude of the house\n4. Whether the house has a basement\n\nYou have historic data to train and validate the model.\n\nWhich of the features is most likely to be a source of leakage?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the line below with one of 1, 2, 3 or 4.\npotential_leakage_feature = 2","execution_count":64,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`# Fill in the line below with one of 1, 2, 3 or 4.\npotential_leakage_feature = 2`"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nLeakage is a hard and subtle issue. You should be proud if you picked up on the issues in these examples.\n\nNow you have the tools to make highly accurate models, and pick up on the most difficult practical problems that arise with applying these models to solve real problems.\n\nThere is still a lot of room to build knowledge and experience. Try out a [Machine Learning Competition](https://www.kaggle.com/competitions) or look through our [Datasets](https://kaggle.com/datasets) to practice your new skills.\n\nAgain, Congratulations!\n\n\n\n---\n**[Intermediate Machine Learning Home Page](https://www.kaggle.com/learn/intermediate-machine-learning)**\n\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}